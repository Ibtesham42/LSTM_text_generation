{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "120136a1-f8c7-4235-8749-4d98e8b32464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Project\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "865e43a9-8c4e-40a6-9ea0-0d168f8b5469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the project gutenberg ebook of the jungle book\\n    \\nthis ebook is for the use of anyone anywhere in the united states and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever. you may copy it, give it away or re-use it under the terms\\nof the project gutenberg license included with this ebook or online\\nat www.gutenberg.org. if you are not located in the united states,\\nyou will have to check the laws of the country where you are located\\nbefore using this ebook.\\n\\ntitle: the jungle book\\n\\nauthor: rudyard kipling\\n\\nrelease date: january 16, 2006 [ebook #236]\\n                most recently updated: may 1, 2023\\n\\nlanguage: english\\n\\n\\n\\n*** start of the project gutenberg ebook the jungle book ***\\n\\n\\n\\n\\nthe jungle book\\n\\nby rudyard kipling\\n\\n\\n\\ncontents\\n\\n     mowgli’s brothers\\n     hunting-song of the seeonee pack\\n     kaa’s hunting\\n     road-song of the bandar-log\\n     “tiger! tiger!”\\n      mowgli’s song\\n     the white seal\\n     lukannon\\n     “rikki-tikki-tavi”\\n      darzee'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'plain.txt'\n",
    "raw_text = open(filename,'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "raw_text[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d7e0924-a333-4932-a07e-f0e47588b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Text Cleaning\n",
    "# import re\n",
    "# corpus = []\n",
    "# for i in range(len(raw_text)):\n",
    "#     rp = re.sub('[^a-zA-Z]', \" \",raw_text[i])\n",
    "#     rp = rp.lower()\n",
    "#     rp = rp.split()\n",
    "#     rp = \" \".join(rp)\n",
    "#     corpus.append(rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0699d32c-51b1-4ee4-8dcd-d14c925ef934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Numbers\n",
    "raw_text = ''.join(i for i in raw_text if not i.isdigit())\n",
    "\n",
    "# Total Character \n",
    "chars = sorted(list(set(raw_text)))\n",
    "\n",
    "char_to_int = dict((c,i) for i,c in enumerate(chars))\n",
    "\n",
    "int_to_char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52edf554-f537-4ed8-b817-7743be59c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Character in Text, Corpus Length 296196\n",
      "Total Vocabulary 55\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print('Total Character in Text, Corpus Length',n_chars)\n",
    "print('Total Vocabulary',n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7233f18a-5954-438c-8043-ce104a86365c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequence  24677\n"
     ]
    }
   ],
   "source": [
    "seq_length = 80  # length of each i/p sequence\n",
    "step = 12 # instead of moving 1 letter at time try skipping few\n",
    "sentence = [] # Xvalues\n",
    "next_chars =[] # Y_Values\n",
    "\n",
    "for i in range(0,n_chars -seq_length ,step):\n",
    "    sentence.append(raw_text[i:i+seq_length])  # Sentence in\n",
    "    next_chars.append(raw_text[i + seq_length]) # Sentence out\n",
    "n_patterns = len(sentence)\n",
    "print(\"Number of sequence \",n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b67233d-ea65-4e3e-9f17-27df1232a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# cv = CountVectorizer()\n",
    "# X = cv.fit_transform(sentence,seq_length).toarray()\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dddfe430-ed71-4dff-917c-c76360146e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24677, 80, 55)\n",
      "(24677, 55)\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(sentence), seq_length, n_vocab), dtype=np.bool_)\n",
    "y = np.zeros((len(sentence), n_vocab), dtype=np.bool_)\n",
    "for i, sentences in enumerate(sentence):\n",
    "    for j, char in enumerate(sentences):\n",
    "        X[i,j, char_to_int[char]]=1\n",
    "    y[i,char_to_int[next_chars[i]]] = 1\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfe9d770-6315-450a-9c77-774cd8ddc880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yaha tumhara seq_length, n_vocab already defined hone chahiye\n",
    "\n",
    "def create_model(optimizer=\"rmsprop\", learning_rate=0.001, dropout_rate=0.3):\n",
    "    if optimizer == \"adam\":\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(129, input_shape=(seq_length, n_vocab), return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(130))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(n_vocab, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3be603e-d5a1-4b59-997b-70fb600a0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # define the checkpoint\n",
    "# # !pip install scikeras\n",
    "# !pip install --upgrade \"scikit-learn>=1.4.1post1\" scikeras\n",
    "\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "\n",
    "filepath=\"saved_weights/saved_weights-{epoch:02d}-{loss:.4f}.keras\"\n",
    "checkpoint = ModelCheckpoint(filepath,monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# 2. Estimator for GridSearchCV\n",
    "estimator = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# 3. Improved param_grid\n",
    "param_grid = {\n",
    "    'batch_size': [32, 64, 128,150],\n",
    "    'epochs': [20, 50,150],\n",
    "    'optimizer': ['adam', 'rmsprop']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=estimator,\n",
    "                    param_grid=param_grid,\n",
    "                    n_jobs=-1,\n",
    "                    cv=5)\n",
    "\n",
    "# 4. Fit with checkpoint callback\n",
    "history = grid.fit(X, y, callbacks=callbacks_list)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best score :\", grid.best_score_)\n",
    "# Fit the model\n",
    "\n",
    "# history = model.fit(X, y,\n",
    "#           batch_size=128,\n",
    "#           epochs=100,   \n",
    "#           callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "best_model = grid.best_estimator_.model_\n",
    "best_model.save(\"my_saved_weights_jungle_book_best.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aeb4db-4173-4a75-a434-220fb6c591e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b297c0d6-27cf-45bc-944f-12972c56c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)\n",
    "    exp_preds = np.exp(preds) #exp of log (x), isn't this same as x??\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1) \n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bcb0f1-6c42-4cc4-aa7c-5ab9d9f2e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "# load the network weights\n",
    "filename = \"my_saved_weights_jungle_book_50epochs.h5\"\n",
    "model.load_weights(filename)\n",
    "\n",
    "#Pick a random sentence from the text as seed.\n",
    "start_index = random.randint(0, n_chars - seq_length - 1)\n",
    "\n",
    "#Initiate generated text and keep adding new predictions and print them out\n",
    "generated = ''\n",
    "sentence = raw_text[start_index: start_index + seq_length]\n",
    "generated += sentence\n",
    "\n",
    "print('----- Seed for our text prediction: \"' + sentence + '\"')\n",
    "#sys.stdout.write(generated)\n",
    "\n",
    "\n",
    "for i in range(400):   # Number of characters including spaces\n",
    "    x_pred = np.zeros((1, seq_length, n_vocab))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds)\n",
    "    next_char = int_to_char[next_index]\n",
    "\n",
    "    generated += next_char\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2275ca1a-6cb4-4a68-ac35-043f4f2b9a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68380a41-b07e-4dc4-9cc5-1a30fe30fe3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2522caa-14a5-4ebc-a01d-b53fbe00be2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c60ae7-8622-4866-996e-9a124f53e2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3292c-c8a3-48ea-a694-b402b93de85b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc64c0e-97e0-42ea-b6aa-0128611a11b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08cd37-e72a-4a88-8a05-b7551faf0d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
